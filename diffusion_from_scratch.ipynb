{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the forward diffusion process\n",
    "\n",
    "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually.\n",
    "\n",
    "The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps T. This happens according to a variance schedule. The original DDPM authors employed a linear schedule:\n",
    "\n",
    "> We set the forward process variances to constants increasing linearly from  ùõΩ1=1e‚àí4  to  ùõΩùëá=0.02\n",
    "\n",
    "However, it was shown in (Nichol et al., 2021) that better results can be achieved when employing a cosine schedule.\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "- The noise-levels/variances can be pre-computed\n",
    "- There are different types of variance schedules\n",
    "- We can sample each timestep image independently (Sums of Gaussians is also Gaussian)\n",
    "- No model is needed in this forward step\n",
    "\n",
    "Defining various variance schedules below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for various variance schedules\n",
    "def linear_beta_schedule(timesteps, beta_start = 0.0001, beta_end = 0.02):\n",
    "    \"\"\" linear schedule as used in the ddpm paper \"\"\"\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\" cosine schedule as proposed in https://arxiv.org/abs/2102.09672 \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps, beta_start = 0.0001, beta_end = 0.02):\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, beta_start = 0.0001, beta_end = 0.02):\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, let's use the linear schedule for T = 250  time steps and define the various variables from the beta_t which we will need, such as the cumulative product of the variances alpha_bar_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 250\n",
    "\n",
    "# defining beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# pre-calculating different terms for closed form\n",
    "# each of the variables below are just 1-dimensional tensors, storing values from t to T \n",
    "## defining alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "## calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "## calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract_index(vals, t, x_shape):\n",
    "    \"\"\" returns the appropriate index t for a batch of indices \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
